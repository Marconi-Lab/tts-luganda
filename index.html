<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Luganda TTS Audio Samples</title>


  <style>
    body {
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      margin: 40px;
      background-color: #f9f9f9;
      color: #333;
    }
  
    h1 {
      text-align: center;
      color: #222;
      margin-bottom: 30px;
    }
  
    table {
      width: 100%;
      border-collapse: collapse;
      box-shadow: 0 0 15px rgba(0, 0, 0, 0.05);
      background-color: white;
      table-layout: fixed; /* ðŸ‘ˆ Ensures columns obey width rules */
    }
  
    th, td {
      padding: 15px;
      text-align: left;
      border-bottom: 1px solid #eee;
      vertical-align: top;
    }
  
    th {
      background-color: #f2f2f2;
      font-weight: 600;
    }
  
    td:nth-child(2) {
      max-width: 300px; /* ðŸ‘ˆ Limit Sentence column */
      overflow-wrap: break-word;
      word-wrap: break-word;
      text-overflow: ellipsis;
    }
  
    audio {
      width: 100%;
      max-width: 250px;
    }
  
    tr:hover {
      background-color: #f9f9f9;
    }
  
    @media (max-width: 768px) {
      table, thead, tbody, th, td, tr {
        display: block;
      }
  
      tr {
        margin-bottom: 20px;
        border: 1px solid #ddd;
        border-radius: 5px;
        padding: 10px;
      }
  
      td {
        padding: 10px 5px;
      }
  
      td:before {
        content: attr(data-label);
        font-weight: bold;
        display: block;
        margin-bottom: 5px;
      }
  
      th {
        display: none;
      }
    }
  </style>
  
</head>
<body>
  <h1>Checkpoints Comparison</h1>
  <p style="max-width: 800px; margin: 0 auto 30px auto; font-size: 1rem; line-height: 1.6;">
    This comparison showcases outputs from a fine-tuned <strong>VITS</strong> (Variational Inference with Adversarial Learning for End-to-End Text-to-Speech) model. The model was pre-trained on female Luganda speech from the <strong>Common Voice</strong> dataset and further fine-tuned on <strong>2.8 hours</strong> of professionally recorded studio data from a single female Luganda speaker. The samples below illustrate how different checkpoints perform on the same input sentences.
  </p>

  <table>
    <thead>
      <tr>
        <!-- <th>ID</th> -->
        <th>Sentence</th>
        <th>Model 1</th>
        <th>Model 2</th>
        <th>Model 3</th>
      </tr>
    </thead>
    <tbody id="myData"></tbody>
  </table>

  <script>
    const tableBody = document.getElementById("myData");
    const url = "test_sentences.json";

    fetch(url)
      .then(response => response.json())
      .then(data => {
        data.forEach(sentence => {
          const row = document.createElement("tr");
          row.innerHTML = `
            
            <td data-label="Sentence">${sentence.sentence}</td>
            <td data-label="Model 1"><audio controls preload="none" src="samples/Model_1/${sentence.id}.wav"></audio></td>
            <td data-label="Model 2"><audio controls preload="none" src="samples/Model_2/${sentence.id}.wav"></audio></td>
            <td data-label="Model 3"><audio controls preload="none" src="samples/Model_3/${sentence.id}.wav"></audio></td>
          `;
          tableBody.appendChild(row);
        });
      })
      .catch(error => {
        console.error("Error fetching data:", error);
      });
  </script>
</body>
</html>
